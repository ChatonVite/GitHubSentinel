{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Box64 and RISC-V in 2024: What It Takes to Run the Witcher 3 on RISC-V (Score: 72)\n",
      "   By: pabs3, Time: 1724732607\n",
      "   URL: https://box86.org/2024/08/box64-and-risc-v-in-2024/\n",
      "\n",
      "2. Notes on Buttondown.com (Score: 18)\n",
      "   By: luu, Time: 1724736352\n",
      "   URL: https://jmduke.com/posts/microblog/buttondown-dot-com/\n",
      "\n",
      "3. FreeBSD-rustdate, a reimplementation of FreeBSD-update (Score: 30)\n",
      "   By: ggm, Time: 1724730395\n",
      "   URL: https://rustdate.over-yonder.net/\n",
      "\n",
      "4. Erasure Coding for Distributed Systems (Score: 193)\n",
      "   By: eatonphil, Time: 1724702625\n",
      "   URL: https://transactional.blog/blog/2024-erasure-coding\n",
      "\n",
      "5. Dokku: My favorite personal serverless platform (Score: 685)\n",
      "   By: tosh, Time: 1724685716\n",
      "   URL: https://hamel.dev/blog/posts/dokku/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_top_hackernews_stories(top_n=5):\n",
    "    # 获取最新的故事ID列表\n",
    "    top_stories_url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
    "    story_ids = requests.get(top_stories_url).json()\n",
    "\n",
    "    top_stories = []\n",
    "    \n",
    "    # 获取前top_n个故事的详细信息\n",
    "    for story_id in story_ids[:top_n]:\n",
    "        story_url = f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
    "        story_details = requests.get(story_url).json()\n",
    "        \n",
    "        if story_details:\n",
    "            top_stories.append({\n",
    "                'title': story_details.get('title'),\n",
    "                'url': story_details.get('url'),\n",
    "                'score': story_details.get('score'),\n",
    "                'by': story_details.get('by'),\n",
    "                'time': story_details.get('time')\n",
    "            })\n",
    "    \n",
    "    return top_stories\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_stories = get_top_hackernews_stories()\n",
    "    for idx, story in enumerate(top_stories, 1):\n",
    "        print(f\"{idx}. {story['title']} (Score: {story['score']})\")\n",
    "        print(f\"   By: {story['by']}, Time: {story['time']}\")\n",
    "        print(f\"   URL: {story['url']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 16:38:10 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-08-27 16:38:10 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.14 (main, Apr 27 2024, 21:17:55) [GCC 13.2.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "2024-08-27 16:38:10 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-08-27 16:38:10 [scrapy.extensions.telnet] INFO: Telnet Password: 4eab7062a39f3a54\n",
      "2024-08-27 16:38:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-08-27 16:38:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 'INFO'}\n",
      "2024-08-27 16:38:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-08-27 16:38:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-08-27 16:38:10 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-08-27 16:38:10 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-08-27 16:38:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-08-27 16:38:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 启动爬虫\u001b[39;00m\n\u001b[1;32m     40\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(HackernewsSpider)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 完\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHubSentinel/.venv/lib/python3.10/site-packages/scrapy/crawler.py:429\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    426\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHubSentinel/.venv/lib/python3.10/site-packages/twisted/internet/base.py:699\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m~/GitHubSentinel/.venv/lib/python3.10/site-packages/twisted/internet/base.py:930\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class HackernewsSpider(scrapy.Spider):\n",
    "    name = 'hackernews_spider'\n",
    "    allowed_domains = ['news.ycombinator.com']\n",
    "    start_urls = ['https://news.ycombinator.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        stories = response.css('tr.athing')\n",
    "        for story in stories:\n",
    "            title_tag = story.find('span', class_='titleline').find('a')\n",
    "            if title_tag:\n",
    "                title = title_tag.css('::text').get()\n",
    "                link = title_tag.css('::attr(href)').get()\n",
    "                yield {\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                }\n",
    "\n",
    "        # 处理下一页的链接\n",
    "        next_page = response.css('a.morelink::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "# 设置爬虫\n",
    "process = CrawlerProcess(settings={\n",
    "    'FEEDS': {\n",
    "        'hackernews_hotspots.json': {\n",
    "            'format': 'json',\n",
    "            'encoding': 'utf8',\n",
    "            'store_empty': False,\n",
    "            'indent': 4,\n",
    "        },\n",
    "    },\n",
    "    'LOG_LEVEL': 'INFO',  # 设置爬虫的日志等级\n",
    "})\n",
    "\n",
    "# 启动爬虫\n",
    "process.crawl(HackernewsSpider)\n",
    "process.start()\n",
    "\n",
    "# 完\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
